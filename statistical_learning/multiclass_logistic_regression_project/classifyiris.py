# -*- coding: utf-8 -*-
"""classifyIris.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16LFv2qTfylAgbvydpMyZIU3Zm4Gsg9dT

# SETTING UP DATA
"""

import pandas as pd
import matplotlib.pyplot as plt
import io
import numpy as np
from sklearn.model_selection import train_test_split
from google.colab import files

def load_iris():
    uploaded = files.upload()
    data = pd.read_csv(io.StringIO(uploaded["Iris.csv"].decode('utf-8')))
    oneHot = pd.get_dummies(data['Species'])
    data = data.drop('Species',axis = 1)
    X = data.values
    Y = oneHot.values
    numExamples = X.shape[0]
    allOnes = np.ones((numExamples,1))
    X = np.concatenate((X,allOnes),axis = 1)
    
    return(X,Y)

#Upload Iris CSV from computer
X,Y = load_iris()

"""# DEFINING FUNCTIONS"""

def soft_max(X,Y,B):
    
    numExamples, numFeatures = X.shape
    numExamples, numClasses = Y.shape
    
    numerator = np.exp(X@B)
    denom = np.sum(numerator,axis =1) 
    denom = np.reshape(denom,(numExamples,1))  
    denom = np.tile(denom,(1,numClasses)) 
    p = numerator/denom
    
    return p
  
  

def eval_f(X,Y,B):
   
  f = np.sum(Y*np.log(soft_max(X,Y,B)))

  return -f

def eval_grad(X,Y,B):
    
    
    numExamples, numFeatures = X.shape
    numExamples, numClasses = Y.shape
    grad = np.zeros((numFeatures,numClasses))
    
    grad = X.T@(soft_max(X,Y,B)-Y)
    
    
    return grad

"""# MULTICLASS LOGISTIC REGRESSION WITH IRIS DATASET"""

#SET UP FOR GRADIENT DESCENT
maxIter = 1000
t = 0.001
#SETTING UP VARIABLES
X_train, X_test,Y_train, Y_test = train_test_split(X, Y, test_size=0.5)
numExamples, numFeatures = X_train.shape
numExamples, numClasses = Y_train.shape
B = np.random.rand(numFeatures,numClasses)

#SETTING UP FOR SEMIOLOGY PLOT
costs = np.zeros(maxIter)
m_iter = np.zeros(maxIter)
#GRADIENT DESCENT FOR TRAIN DATA
for i in range(maxIter):
  B = B - (t*eval_grad(X_train,Y_train,B))
  cost = eval_f(X_train,Y_train,B)
  costs[i] = cost
  m_iter[i] = i
  
  if i%10 == 0:
    print(cost)

"""## Setting Up Semiology Plot"""

#SEMILOGY PLOT
plt.grid(True, which="both")
plt.semilogy(m_iter, costs)
plt.title('Multiclass Logistic Regression with Iris dataset')
plt.xlabel('Iteration')
plt.ylabel('Cost')
plt.show()

"""## Predicting Test Data"""

#Set up predictons matrix
Y_final = np.zeros((numExamples,numClasses),dtype=int)
#Find the probabilities for Y
softmax = soft_max(X_test,Y_final,B)
#Assign the highest probability to = 1
for i in range(Y_final.shape[0]):
  argmax = np.argmax(softmax[i])
  Y_final[i][argmax] = 1

#PREDICTION ACCURACY
correct = 0
incorrect = 0

for i in range(Y_final.shape[0]):
  #print("Predicted: " + str(np.argmax(Y_final[i])) + " Actual: " + str(np.argmax(Y_test[i])))
  final = np.argmax(Y_final[i]) == np.argmax(Y_test[i])
  if (final):
    #print("Yay!")
    correct = correct + 1
  else:
    incorrect = incorrect + 1
    
accuracy = correct/(incorrect + correct)

print("Classification Accuracy: " + str(accuracy*100) + "%")